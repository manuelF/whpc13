\section{Ejercicio 3}

\subsection{Descripci\'on}
El ejercio 3 consiste en completar una fracci\'on de la simulaci\'on de los avances de los frentes magn\'eticos
en interfaces de medios desordenados. Para eso se gener\'o un modelo basado en mec\'anica cl\'asica, usando
\textit{splines} para trazar las curvas de las particulas y \textit{springs} para delimitar el frente.

El c\'odigo fue hecho usando la librer\'ia \texttt{Thrust} para poderlo correr en CPU y GPU indistintamente.
Tambien se utiliz\'o un random number generator especial, basado en contadores, para no tener que almacenar las posiciones 
hist\'oricas de cada particula. Esto hace manejable los tama\~nos de de mem\'oria de modo que entren en las placas
aceleradoras GPGPU. 

\subsection{TODOs}
El primer \texttt{TODO} era relacionado al RNG. Este TODO se\~nalaba que habia un error con respecto al
uso del RNG en el caso del ruido t\'ermico. Habia que entender como funcionaba un counter-based RNG para
poder entender cual era el error ahi. El problema radicaba en como obtener n\'umeros aleatorios no correlacionados
pero que sigan siendo reproducibles de la misma manera que los usados para el desorden del medio. 
Los counter-based RNG toman dos parametros, un counter y una key, y devuelven un numero aleatorio. Llamar
con los mismos parametros al RNG siempre devuelve el mismo n\'umero. Esto indica que para obtener 
el siguiente valor de la secuencia random, habr\'ia que modificar el counter. Sin embargo, quisieramos
obtener valores aleatorios tambien para el otro ruido, y debieramos seedearlos distinto para que no
haya ninguna correlaci\'on entre ellos. Para eso, habria que variar la key, y setearla al seed correspondiente.

Los CBRNG usados aca toman como key el valor del thread donde estan haciendo el computo, y como counter 
la uni\'on de un seed de cada fuente de numeros aleatorios con el $u$ usado, donde $u$ es la posici\'on del mon\'omero
actual. De esta manera los valores random son reproducibles hacia atras y hacia adelante.

Dicho todo esto, el FIXME consistia en entender la teoria de los CBRNG y ver que el uso que se le estaba dando
no generaba valores aleatorios diferentes, sino que los podia repetir. La soluci\'on consistia en hacer que el ruido
t\'ermico dependiera del tiempo de la simulaci\'on, es decir, que no pueda volver para atras el ruido termino.
Esto se hacia duplicando el mecanismo usado anteriormente y cambiando el seed al seed2 y el $u$ por el $t$ de la simulaci\'on.


El segundo \texttt{TODO} se referia al c\'alculo de dos propiedades de la interfaz, la velocidad y la posicion del centro de masas.
Para calcular el centro de masas, debemos obtener la posicion de cada uno de los
puntos y promediarla. Como el problema son $L$ puntos equiespaciados en la coordenada $y$, tomamos solamente los valores 
de X para obtener el centro de masas. Esto se traduce en un c\'odigo de thrust de reduce asi: \\
\texttt{ REAL center\_of\_mass = reduce(u\_it0, u\_it1, 0.0) / L;}\\
Esto hace una suma de todos las posiciones en X de las particulas y las promedia al dividirlas por L (fuera de la placa).

Para obtener la velocidad del centro de masas, deberiamos ver la sumatoria de las fuerzas del sistema, para ver en que
direcci\'on y con que magnitud se esta moviendo. Luego el c\'odigo relevante es:\\
\newcommand*\justify{%
  \fontdimen2\font=0.4em% interword space
  \fontdimen3\font=0.2em% interword stretch
  \fontdimen4\font=0.1em% interword shrink
  \fontdimen7\font=0.1em% extra space
  \hyphenchar\font=`\-% allowing hyphenation
}
\texttt{\justify{REAL velocity = reduce(Ftot\_it0, Ftot\_it1, (REAL(0.0)))/((REAL)L);
                             }}\\
\subsection{Resultados}
 \begin{figure}[H]
 \begin {center}
 \includegraphics[width=\hrwidth]{plots/ej3omp.png}
 \end {center}
 \caption{Runtime del ej3 en Thrust con OMP, en funcion de la cantidad de threads, para 10k iteraciones temporales}
 \label{fig:ej3OMP}
 \end{figure}

 \begin{table} [H]
    \begin{tabular}{l|r|r|r}
        \textbf{M\'etodo} & \textbf{ Runtime Total [ms]}& \textbf{Tiempo fuerzas [ms]} & \textbf{Tiempo euler [ms]}\\ \hline
        1 OMP Thread         & 15861.80      & 15240.20  & 577.91\\
   2 OMP Thread          & 7971.23     & 7654.43 &309.73  \\
   4 OMP Thread          & 4072.82     & 3888.24 & 162.07 \\
   8 OMP Thread  & 2235.89     & 2130.46 & 111.05 \\
   16 OMP Thread & 1249.57     & 1132.09 & 93.87 \\
       CUDA M2090 & 473.18  & 290.505 & 176.06
   \end{tabular}
   
\end{table}


En el gr\'afico de la figura \ref{fig:ej3OMP} junto a la tabla correspondiente, podemos apreciar que este problema escala
increiblemente bien. En efecto, como el c\'alculo de casi todo es independiente, y solo se comparten pocos datos en un momento,
el problema puede escalar casi linealmente. La placa nVidia, sin embargo, sigue siendo un orden de magnitud m\'as rapida que
lo m\'aximo que podemos alcanzar usando un solo procesador Xeon en el servidor.
Comparando en la tabla con la placa GPGPU, extrapolando, podriamos concluir que equivale a un procesador de mas de 32 threads. 
Seria interesante probar este experimento en una Xeon Phi, a ver si realmente se puede comparar la performance de ambos dispositivos,
como dicen los fabricantes.

Un detalle interesante surge de ver la discrepancia en porcentaje de tiempo invertidos en las distintas cuentas. Ac\'a se puede apreciar
la potencia pura de la placa; el fragmento de codigo de fuerzas se acelera muchi\'simo. Esto se puede aplicar tanto por el hecho de
que se realizan las cuentas totalmente paralelas en distintos threads y que ademas se aprovecha el fuerte de la placa que son
las instrucciones sin condicionales. De haber algun if en ese codigo, por minusculo que sea, los valores de runtime serian bastante
mas desfavorables. Sin embargo, en el c\'alculo de paso de euler, es mucho peor la performance comparado contra incluso 4 threads OMP.
Es de creer que esto puede ser porque en si son bastantes pocas cuentas; $2^{16}$ elementos son relativamente pocos para tener 
tan baja intensidad aritm\'etica. Esto hace que se preste mejor la opcion vectorizada de calculos comparado contra la opcio\'n 
many threads de CUDA. 
